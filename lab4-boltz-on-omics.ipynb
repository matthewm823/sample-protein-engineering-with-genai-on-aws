{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 (inference): Running Nextflow pipelines with HealthOmics Workflows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to create, run, and debug Nextflow based pipelines that process data from HealthOmics Storage and Amazon S3 using HealthOmics Workflows."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4550118a",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "### Python requirements\n",
    "* Python >= 3.8\n",
    "* Packages:\n",
    "  * boto3 >= 1.26.19\n",
    "  * botocore >= 1.29.19\n",
    "\n",
    "### AWS requirements\n",
    "\n",
    "#### AWS CLI\n",
    "You will need the AWS CLI installed and configured in your environment. Supported AWS CLI versions are:\n",
    "\n",
    "* AWS CLI v2 >= 2.9.3 (Recommended)\n",
    "* AWS CLI v1 >= 1.27.19\n",
    "\n",
    "#### Output buckets\n",
    "You will need a bucket **in the same region** you are running this tutorial in to store workflow outputs.\n",
    "\n",
    "#### Input data\n",
    "If you modify any of the workflows to retrieve input data (e.g. references or raw reads), that data **MUST reside in the same region**. AWS HealthOmics does not support cross-region read or write at this time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "First, let's get our AWS account information and set up variables we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account ID: 648913559821\n",
      "Region: us-east-1\n",
      "S3 Bucket: nimbustx-boltz2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import io\n",
    "import os\n",
    "from pprint import pprint\n",
    "from textwrap import dedent\n",
    "from time import sleep\n",
    "from urllib.parse import urlparse\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "\n",
    "import boto3\n",
    "import botocore.exceptions\n",
    "\n",
    "# Get AWS account information\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Define S3 bucket and folder names\n",
    "S3_BUCKET = f'nimbustx-boltz2'\n",
    "LAB1_FOLDER = 'lab1-progen'\n",
    "LAB2_FOLDER = 'lab2-amplify'\n",
    "LAB3_FOLDER = 'lab3-esmfold'\n",
    "LAB4_FOLDER = 'lab4-boltz'\n",
    "\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: {S3_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b2f309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local data folders for lab 4 \n",
    "!mkdir -p data/$LAB4_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d4d587",
   "metadata": {},
   "source": [
    "### Upload Model file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync data/{LAB4_FOLDER}/model s3://{S3_BUCKET}/{LAB4_FOLDER}/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b2bea",
   "metadata": {},
   "source": [
    "### Modify Nextflow Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d351cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"boltz/definition/nextflow.config\"\n",
    "\n",
    "try:\n",
    "    with open(filename, 'r') as file:\n",
    "        file_content = file.read()\n",
    "\n",
    "    modified_content = file_content.replace('ModelS3Location', f\"s3://{S3_BUCKET}/{LAB4_FOLDER}/model/\")\n",
    "    modified_content = modified_content.replace('ECRImageURI', f\"{account_id}.dkr.ecr.{region}.amazonaws.com/boltz2:latest\")\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(modified_content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{filename}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find IAM Role to run job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.iam_helper import IamHelper\n",
    "\n",
    "# IAM role for ESMFold run\n",
    "iam_helper = IamHelper()\n",
    "job_role_arn = iam_helper.find_role_arn_by_pattern('OmicsWorkflowRole')\n",
    "\n",
    "print(f'Job role ARN: {job_role_arn}') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Using AWS HealthOmics Workflows - the basics\n",
    "AWS HealthOmics Workflows allows you to perform bioinformatics compute - like genomics secondary analysis - at scale on AWS. These compute workloads are defined using workflow languages like WDL and Nextflow that specify multiple compute tasks and their input and output dependencies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this workflow, we'll start by creating a client for the `omics` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics = boto3.client('omics')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to bundle up the workflow as a zip-file and call the `create_workflow` API for `omics`.  We'll encapsulate these operations in a function for reuse later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workflow(\n",
    "    workflow_root_dir, \n",
    "    parameters={\"param_name\":{\"description\": \"param_desc\"}}, \n",
    "    name=None, \n",
    "    description=None, \n",
    "    main=None):\n",
    "    buffer = io.BytesIO()\n",
    "    print(\"creating zip file:\")\n",
    "    with ZipFile(buffer, mode='w', compression=ZIP_DEFLATED) as zf:\n",
    "        for file in glob.iglob(os.path.join(workflow_root_dir, '**/*'), recursive=True):\n",
    "            if os.path.isfile(file):\n",
    "                arcname = file.replace(os.path.join(workflow_root_dir, ''), '')\n",
    "                print(f\".. adding: {file} -> {arcname}\")\n",
    "                zf.write(file, arcname=arcname)\n",
    "\n",
    "    response = omics.create_workflow(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        definitionZip=buffer.getvalue(),  # this argument needs bytes\n",
    "        main=main,\n",
    "        parameterTemplate=parameters,\n",
    "    )\n",
    "\n",
    "    workflow_id = response['id']\n",
    "    print(f\"workflow {workflow_id} created, waiting for it to become ACTIVE\")\n",
    "\n",
    "    try:\n",
    "        waiter = omics.get_waiter('workflow_active')\n",
    "        waiter.wait(id=workflow_id)\n",
    "\n",
    "        print(f\"workflow {workflow_id} ready for use\")\n",
    "    except botocore.exceptions.WaiterError as e:\n",
    "        print(f\"workflow {workflow_id} FAILED:\")\n",
    "        print(e)\n",
    "\n",
    "    workflow = omics.get_workflow(id=workflow_id)\n",
    "    return workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to notice:\n",
    "\n",
    "* To avoid polluting the local filesystem the zip-file is created in-memory with a byte buffer. If your workflow has many files such that the resultant bundle is large, you should consider alternative means of creating the zip file.\n",
    "* A `main.(ext)` file, where `ext` matches the type of the workflow (e.g. `wdl`, or `nf`) must be at the root level of the zip file. HealthOmics uses this file as the primary entry point for the workflow. This is relevant for more modular workflows that have multiple definition files. In the call below, we explicitly point to `main.wdl`.\n",
    "* The `definitionZip` argument takes a binary value and reads the byte buffer value directly.\n",
    "* The `parameters` argument is a list of `parameterTemplate`s, which for now provide the parameter's name, and a description of what the parameter is. Actual parameter values are provided when the workflow is \"run\" - more on this below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this function to create a workflow in HealthOmics Workflows from our WDL definition above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = create_workflow(\n",
    "    'boltz/definition', \n",
    "    parameters={\"input_path\": {\"description\": \"Path to fasta or yaml input file.\"}},\n",
    "    name=\"Boltz2\",\n",
    "    description=\"Sample Boltz2 workflow\",\n",
    "    main=\"main.nf\"\n",
    ")\n",
    "pprint(workflow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start a workflow run with some input data using the `start_run` API call.\n",
    "\n",
    "Note the following:\n",
    "* Here the parameter value `input_file` is associated with an S3 uri. This is specific to this case. Workflow parameters will vary depending on the workflow definition they are associated with.\n",
    "\n",
    "* We provide the ARN to the service role we created above. You can specify different roles as needed depending on what resources your workflow needs access to.\n",
    "\n",
    "* We provide an `outputUri` with `start_run`. This is where the workflow will place **final** outputs as they are defined by the workflow definition (e.g. values in the `workflow.output` block of a WDL workflow). All intermediate results are discarded once the workflow completes.\n",
    "\n",
    "In the cell below, we're using `waiters` to check for when the run starts and completes. These will block the current execution thread.\n",
    "\n",
    "It will take about **30 minutes** for this workflow to start (scale up resources), run, and stop (scale down resources). Because this workflow is simple, the time it spends in a `RUNNING` state is fairly short relative to the scale-up/down times. For more complex workflows, or ones that process large amounts of data, the `RUNNING` state will be much longer (e.g. several hours). In that case, it's recommended to asynchronously check on the workflow status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp boltz/input/example.fa s3://{S3_BUCKET}/{LAB4_FOLDER}/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_uri = f\"s3://{S3_BUCKET}/{LAB4_FOLDER}/input/example.fa\"\n",
    "output_uri = f\"s3://{S3_BUCKET}/{LAB4_FOLDER}/results/\"\n",
    "\n",
    "run = omics.start_run(\n",
    "    workflowId=workflow['id'],\n",
    "    name=\"Sample workflow run\",\n",
    "    roleArn=job_role_arn,\n",
    "    parameters={\n",
    "        \"input_path\": input_uri\n",
    "    },\n",
    "    outputUri=output_uri,\n",
    ")\n",
    "\n",
    "print(f\"running workflow {workflow['id']}, starting run {run['id']}\")\n",
    "try:\n",
    "    waiter = omics.get_waiter('run_running')\n",
    "    waiter.wait(id=run['id'], WaiterConfig={'Delay': 30, 'MaxAttempts': 60})\n",
    "\n",
    "    print(f\"run {run['id']} is running\")\n",
    "\n",
    "    waiter = omics.get_waiter('run_completed')\n",
    "    waiter.wait(id=run['id'], WaiterConfig={'Delay': 60, 'MaxAttempts': 60})\n",
    "\n",
    "    print(f\"run {run['id']} completed\")\n",
    "except botocore.exceptions.WaiterError as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the run completes we can verify its status by either listing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_ for _ in omics.list_runs()['items'] if _['id'] == run['id']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or getting its full details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics.get_run(id=run['id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the correct output was generated by listing the `outputUri` for the workflow run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3uri = urlparse(omics.get_run(id=run['id'])['outputUri'])\n",
    "prefix = f\"{s3uri.path[1:]}{run['id']}\"\n",
    "boto3.client('s3').list_objects_v2(\n",
    "    Bucket=s3uri.netloc, \n",
    "    Prefix=prefix\n",
    ")['Contents']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflows typically have multiple tasks. We can list workflow tasks with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = omics.list_run_tasks(id=run['id'])\n",
    "pprint(tasks['items'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and get specific task details with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = omics.get_run_task(id=run['id'], taskId=tasks['items'][0]['taskId'])\n",
    "pprint(task)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell above we should see that each task has an associated CloudWatch Logs LogStream. These capture any text generated by the workflow task that has been sent to either `STDOUT` or `STDERR`. These outputs are helpful for debugging any task failures and can be retrieved with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = boto3.client('logs').get_log_events(\n",
    "    logGroupName=\"/aws/omics/WorkflowLog\",\n",
    "    logStreamName=f\"run/{run['id']}/task/{task['taskId']}\"\n",
    ")\n",
    "for event in events['events']:\n",
    "    print(event['message'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
